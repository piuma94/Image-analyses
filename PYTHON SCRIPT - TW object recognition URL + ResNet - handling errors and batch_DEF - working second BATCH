

### this code works particularly well in visual studio if each line is sent on it's own so use shift e invio to send each line, at some point the code will start


import torch
import torchvision
import requests
import gc
from torchvision.models.detection import FasterRCNN
from torchvision.transforms import functional as F
from transformers import YolosFeatureExtractor, YolosForObjectDetection, AutoImageProcessor
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
from transformers import pipeline
import os
import tqdm
import pandas as pd
from transformers import AutoImageProcessor, ResNetForImageClassification
import socket
import time

# Read the CSV file with image URLs
#csv_file = "C:/Users/luigi/OneDrive - University of Glasgow/Desktop/Preliminary Script/twiit&URL for colab_14_07_2023.csv"
## # Example of specifying an absolute path
# Print the current working directory
print("Current working directory:", os.getcwd())
# Change the working directory if needed
os.chdir("/Users/luigi.caopinna/Library/CloudStorage/OneDrive-UniversityofGlasgow/Desktop/Pipelines - DEF script/flicker/Python - image analyses/2024-08-14 python results")
print("New working directory:", os.getcwd()) ## i want to save all these results in the separate folder

## THE FOLDER IN WHICH I SAVE AND THE IMAGES URL I ELABORATE SHOULD BE THE SAME AND MATCHED


csv_file = "/Users/luigi.caopinna/Library/CloudStorage/OneDrive-UniversityofGlasgow/Desktop/Pipelines - DEF script/flicker/Python - image analyses/_2024-08-14_flick_uniq_URLs_.csv" ## this is the initial retrieval
df = pd.read_csv(csv_file)

# Create a list to store the results
results = []
long_format_results = []

# Load the pre-trained Faster R-CNN model
model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model_rcnn.eval()

# Get the COCO label names
COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',
    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',
    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',
    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',
    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

# Load the YOLOS model for object detection
feature_extractor_yolos = YolosFeatureExtractor.from_pretrained('hustvl/yolos-base')
model_yolos = YolosForObjectDetection.from_pretrained('hustvl/yolos-base')
image_processor_yolos = AutoImageProcessor.from_pretrained("hustvl/yolos-base")

# Load the BLIP model for image captioning
processor_blip = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base") ## this is the base 
model_blip = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
#processor_blip = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-large") ## this is a try with large and eventually using gpus
#model_blip = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-large")

# Load the image-to-text model
image_to_text = pipeline("image-to-text", model="nlpconnect/vit-gpt2-image-captioning")

# Load the ResNet-50 model for image classification
classification_processor = AutoImageProcessor.from_pretrained("microsoft/resnet-50")
classification_model = ResNetForImageClassification.from_pretrained("microsoft/resnet-50")

# Process each image URL
batch_size = 10000  # Set the batch size to 10 images
results = []
retry_limit = 6  # Number of retries
retry_delay = 1800  # 1800 Delay between retries in seconds ## this is of 30 minutes

# Check if there is a saved batch index
if os.path.exists("batch_index.txt"):
    with open("batch_index.txt", "r") as file:
        batch_index = int(file.read())
else:
    batch_index = 0

for i in range(batch_index, len(df), batch_size):
    batch_urls = df["x"][i:i+batch_size]

    for image_url in tqdm.tqdm(batch_urls):
        # Check internet connection (optional)
        #try:
        #    socket.create_connection(("8.8.8.8", 53), timeout=5)
        #except OSError:
        #    print("Lost internet connection. Waiting for 0.5 minute...")
        #    time.sleep(30)  # Wait for 10 minutes (600 seconds)
        #    continue  # Continue to the next iteration of the loop

        ## a preliminary try, to get if we are offline 
        attempt_count = 0
        ### this part is to handle a generic error i may get while processing
        ## we may expect a connection error happening, but the truth is that the error we get is not of connection, but another one as well, so we did a stop on the general error, eventually.
        while attempt_count < retry_limit:
            try:
            # Attempt to download and open the image
                response = requests.get(image_url, stream=True)
                
                break
            # If download is successful, break out of the retry loop


            except Exception as e:
                attempt_count += 1
                print(f"Connection error encountered on attempt {attempt_count} for {image_url}: {e}")
            
                if attempt_count < retry_limit:
                    print(f"Pausing for {retry_delay//60} minutes before retrying...")
                    time.sleep(retry_delay)
                else:
                    print("Reached maximum retry attempts. Moving to the next image.")
                    break  # Exit the loop if max retry limit is reached

            


        try:
            # Download and open the image
            response = requests.get(image_url, stream=True)
            response.raise_for_status()
            image = Image.open(response.raw)

            # Convert image to tensor
            image_tensor = F.to_tensor(image)

            # Perform object detection using Faster R-CNN
            with torch.no_grad():
                predictions = model_rcnn([image_tensor])
            labels = predictions[0]['labels'].tolist()
            boxes = predictions[0]['boxes'].tolist()
            scores = predictions[0]['scores'].tolist()

            # Retrieve label names for Faster R-CNN
            filtered_labels = [COCO_INSTANCE_CATEGORY_NAMES[label] for label in labels]

            # Perform object detection using YOLOS
            inputs_yolos = feature_extractor_yolos(images=image, return_tensors="pt")
            outputs_yolos = model_yolos(**inputs_yolos)
            del inputs_yolos
            target_sizes = torch.tensor([image.size[::-1]])
            yolos_results = image_processor_yolos.post_process(outputs_yolos, target_sizes=target_sizes)[0]
            yolos_labels = [model_yolos.config.id2label[label.item()] for label in yolos_results["labels"]]
            yolos_scores = yolos_results["scores"].tolist()
            yolos_boxes = yolos_results["boxes"].tolist()

            # Perform image captioning using BLIP
            raw_image = Image.open(requests.get(image_url, stream=True).raw).convert('RGB')
            inputs_blip = processor_blip(raw_image, return_tensors="pt")
            outputs_blip = model_blip.generate(**inputs_blip)
            caption_blip = processor_blip.decode(outputs_blip[0], skip_special_tokens=True)

            # Perform image-to-text using the Vit-GPT2 model
            text = image_to_text(image)[0]['generated_text']

            # Image classification with ResNet-50
            inputs_classification = classification_processor(image, return_tensors="pt")
            outputs_classification = classification_model(**inputs_classification)
            predicted_label_idx = outputs_classification.logits.argmax(dim=1).item()
            predicted_scores = outputs_classification.logits.softmax(dim=1)[0].tolist()

            logits = outputs_classification.logits
        
            predicted_label = logits.argmax(-1).item()

            # Store the results
            results.append({
                "Image_URL": image_url,
                "FasterRCNN_Labels": filtered_labels,
                "FasterRCNN_Boxes": boxes,
                "FasterRCNN_Scores": scores,
                "YOLOS_Labels": yolos_labels,
                "YOLOS_Boxes": yolos_boxes,
                "YOLOS_Scores": yolos_scores,
                "Caption_blip": caption_blip,
                "Text_gpt2": text,
                "ResNet50_Label": classification_model.config.id2label[predicted_label],
                "ResNet50_Scores": predicted_scores[predicted_label]
            })

            # Append the Faster R-CNN results to the long format
            for label, box, score in zip(filtered_labels, boxes, scores):
                long_format_results.append({
                    "image_path": image_url,
                    "label": label,
                    "box": box,
                    "score": score,
                    "method": "FasterRCNN"
                })

            # Append the YOLOS results to the long format
            for label, box, score in zip(yolos_labels, yolos_boxes, yolos_scores):
                long_format_results.append({
                    "image_path": image_url,
                    "label": label,
                    "box": box,
                    "score": score,
                    "method": "YOLOS"
                })

            # Append the caption and text results to the long format
            long_format_results.append({
                "image_path": image_url,
                "label": None,
                "box": None,
                "score": None,
                "method": "Caption_blip",
                "text": caption_blip
            })

            long_format_results.append({
                "image_path": image_url,
                "label": None,
                "box": None,
                "score": None,
                "method": "Text_gpt2",
                "text": text
            })

            # Append the ResNet-50 classification results to the long format
            resnet = {
                "image_path": image_url,
                "label": classification_model.config.id2label[predicted_label],
                "score": predicted_scores[predicted_label],
                "method": "ResNet50"
            }
            long_format_results.append(resnet)
            print(resnet)

            # Free up memory
            del image, image_tensor, predictions, filtered_labels, boxes, scores
            del outputs_yolos, target_sizes, yolos_results, yolos_labels, yolos_scores, yolos_boxes
            del raw_image, inputs_blip, outputs_blip, caption_blip, text
            gc.collect()

        except Exception as e:
            print(f"Error processing image {image_url}: {str(e)}")
            continue  # Skip the current URL if there's an error

    # Save the batch results after processing the batch
    # Convert the results to a DataFrame
    results_df = pd.DataFrame(results)

    # Save the results to a CSV file
    results_df.to_csv(f"object_detection_results_{i+batch_size}.csv", index=False)
    # Convert the long format results to a DataFrame
    long_format_results_df = pd.DataFrame(long_format_results)

    # Save the long format results to a CSV file with a unique filename
    # indicating the number of images processed so far
    long_format_results_df.to_csv(f"long_format_results_{i+batch_size}.csv", index=False)

    # Update the batch index
    batch_index = i + batch_size
    with open("batch_index.txt", "w") as file:
        file.write(str(batch_index))

    # Clear the lists for the next batch
    results.clear()
    long_format_results.clear()

    # Free up memory
    del results_df, long_format_results_df
    gc.collect()

## SE NON VA, INSISTI, CHIUDI LE PARENTESINE E RIFAI, INSOMMA DEVE FUNZIONARE..... 

## this part do not work and it is useless
# Convert the results to a DataFrame
#results_df = pd.DataFrame(results)

# Save the results to a CSV file
#results_df.to_csv("/content/drive/MyDrive/Colab Notebooks/object_detection_results.csv", index=False)

# Convert the long format results to a DataFrame
#long_format_results_df = pd.DataFrame(long_format_results)

# Save the long format results to a CSV file
#long_format_results_df.to_csv("/Users/luigi.caopinna/Library/CloudStorage/OneDrive-UniversityofGlasgow/Desktop/Pipelines - DEF script/flicker/Python - image analyses/Flicker_object_detection_results_long_format_26_03_2024.csv", index=False)
