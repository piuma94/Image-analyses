#PYTHON SCRIPT - WS object recognition loop + ResNet
import torch
import torchvision
import requests
import gc
from transformers import (
    YolosFeatureExtractor,
    YolosForObjectDetection,
    AutoImageProcessor,
    BlipProcessor,
    BlipForConditionalGeneration,
    pipeline,
    ResNetForImageClassification,
    AutoTokenizer,
)
from torchvision.models.detection import FasterRCNN
from torchvision.transforms import functional as F
from transformers import YolosFeatureExtractor, YolosForObjectDetection, AutoImageProcessor
from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration
import os
import tqdm
import pandas as pd
from transformers import pipeline
from pillow_heif import register_heif_opener

register_heif_opener()

# Define the COCO labels
COCO_INSTANCE_CATEGORY_NAMES = [
    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',
    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',
    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',
    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',
    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',
    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',
    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',
    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]

# Load the pre-trained Faster R-CNN model
model_rcnn = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
model_rcnn.eval()

# Load the YOLOS model for object detection
feature_extractor_yolos = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')
model_yolos = YolosForObjectDetection.from_pretrained('hustvl/yolos-small')
image_processor_yolos = AutoImageProcessor.from_pretrained("hustvl/yolos-small")

# Load the BLIP model for image captioning
processor_blip = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model_blip = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# Load the image-to-text model
image_to_text = pipeline("image-to-text", model="nlpconnect/vit-gpt2-image-captioning")

# Load the ResNet-50 model for image classification
classification_processor = AutoImageProcessor.from_pretrained("microsoft/resnet-50")
classification_model = ResNetForImageClassification.from_pretrained("microsoft/resnet-50")

# Modify the process_image function to handle .heic format images
def process_image(image_path):
    try:
        # Convert HEIF image to JPEG format
        if image_path.lower().endswith('.heic'):
            converted_image_path = image_path[:-5] + '.jpg'
            Image.open(image_path).convert('RGB').save(converted_image_path, 'JPEG')
            image_path = converted_image_path

        # Object detection with Faster R-CNN
        image = Image.open(image_path)
        image_tensor = F.to_tensor(image)
        predictions = model_rcnn([image_tensor])

        boxes = predictions[0]['boxes'].tolist()
        labels = predictions[0]['labels'].tolist()
        scores = predictions[0]['scores'].tolist()

        filtered_labels = [COCO_INSTANCE_CATEGORY_NAMES[label] for label in labels]

        # Object detection with YOLOS
        inputs_yolos = feature_extractor_yolos(images=image, return_tensors="pt")
        outputs_yolos = model_yolos(**inputs_yolos)
        target_sizes = torch.tensor([image.size[::-1]])
        results_yolos = image_processor_yolos.post_process_object_detection(outputs_yolos, threshold=0.05, target_sizes=target_sizes)[0]

        yolos_labels = [model_yolos.config.id2label[label.item()] for label in results_yolos["labels"]]
        yolos_boxes = [box.tolist() for box in results_yolos["boxes"]]
        yolos_scores = [score.item() for score in results_yolos["scores"]]

        # Image captioning with BLIP
        inputs_blip = processor_blip(image, return_tensors="pt")
        out = model_blip.generate(**inputs_blip)
        caption_blip = processor_blip.decode(out[0], skip_special_tokens=True)

        # Image-to-text with ViT-GPT2
        text = image_to_text(image_path)[0]['generated_text']

        
         # Image classification with ResNet-50
        inputs_classification = classification_processor(image, return_tensors="pt")
        outputs_classification = classification_model(**inputs_classification)
        predicted_label_idx = outputs_classification.logits.argmax(dim=1).item()
        predicted_scores = outputs_classification.logits.softmax(dim=1)[0].tolist()

        logits = outputs_classification.logits
        
        predicted_label = logits.argmax(-1).item()

        #print("---- Classification with ResNet50_Label completed ---")
        #print(f"{classification_model.config.id2label[predicted_label]}: {predicted_scores[predicted_label]}")
        #print("---- printing all labels ---")

        #print(f"{classification_model.config.id2label}")
        #print("---- printing all scores ---")

        #print(f"{predicted_scores}")

        # Cleanup memory
        del image, image_tensor, predictions, inputs_yolos, outputs_yolos, target_sizes, results_yolos
        torch.cuda.empty_cache()
        gc.collect()

        return {
            "image_path": image_path,
            "FasterRCNN_Labels": filtered_labels,
            "FasterRCNN_Boxes": boxes,
            "FasterRCNN_Scores": scores,
            "YOLOS_Labels": yolos_labels,
            "YOLOS_Boxes": yolos_boxes,
            "YOLOS_Scores": yolos_scores,
            "Caption_blip": caption_blip,
            "Text_gpt2": text,
            "ResNet50_Label": classification_model.config.id2label[predicted_label],
            "ResNet50_Scores": predicted_scores[predicted_label]
        }

    except Exception as e:
        print(f"Error processing image: {image_path}")
        print(e)
        return None

# Set the main directory containing the sub-folders
main_directory = "C:/Users/luigi/OneDrive - University of Glasgow/Photo walks repository"

# Create a list to store the results in the long format
long_format_results = []

# Get the total number of files for the progress bar
total_files = sum([len(files) for _, _, files in os.walk(main_directory)])

# Initialize the progress bar
pbar = tqdm.tqdm(total=total_files, unit="image")


# import os



# Loop through the sub-folders and process the images
for root, dirs, files in os.walk(main_directory):
    for file in files:
        image_path = os.path.join(root, file)
        result = process_image(image_path)
        if result:
            image_path = result["image_path"]

            # Append the Faster R-CNN results to the long format
            for label, box, score in zip(result["FasterRCNN_Labels"], result["FasterRCNN_Boxes"], result["FasterRCNN_Scores"]):
                long_format_results.append({
                    "image_path": image_path,
                    "label": label,
                    "box": box,
                    "score": score,
                    "method": "Faster R-CNN"
                })

            # Append the YOLOS results to the long format
            for label, box, score in zip(result["YOLOS_Labels"], result["YOLOS_Boxes"], result["YOLOS_Scores"]):
                long_format_results.append({
                    "image_path": image_path,
                    "label": label,
                    "box": box,
                    "score": score,
                    "method": "YOLOS"
                })

            # Append the BLIP caption to the long format
            long_format_results.append({
                "image_path": image_path,
                "caption": result["Caption_blip"],
                "method": "BLIP"
            })

            # Append the ViT-GPT2 text to the long format
            long_format_results.append({
                "image_path": image_path,
                "text": result["Text_gpt2"],
                "method": "ViT-GPT2"
            })

            # Append the Resnet50 text to the long format
            long_format_results.append({
                "image_path": image_path,
                "label":result["ResNet50_Label"],
                "score":result["ResNet50_Scores"],
                "method":"ResNet50"
            })

       

        # Update the progress bar
        pbar.update(1)

# Close the progress bar
pbar.close()

# Create a DataFrame from the long format results
long_format_df = pd.DataFrame(long_format_results)

# Save the results to a CSV file
long_format_df.to_csv('C:/Users/luigi/OneDrive - University of Glasgow/Desktop/Preliminary Script/WS_image_analysis_results_long_format.csv', index=False)

# Print the resulting DataFrame
print(long_format_df)

# In this updated pipeline, I added the Image Classification step using the ResNet-50 model. The predicted label #romesNet-50 is included in the final results.
## I have a new pipeline for twitter, it can be interesting to upload the code also here, to have savings each few images. just to be sure. although it will not be necessary, if the problem was internet and I download all the images ahead. in my local laptop or home